---
id: "google-agent-day4"
topic: "Agent Evaluation"
category: "Quality Quality"
icon: "FaChartLine"
summary: "Day 4: Moving beyond simple metrics to comprehensive Agent Eval."
details: 
  - "LLM as a Judge"
  - "Trajectory Analysis"
  - "Golden Datasets"
  - "Human-in-the-loop Testing"
link: "https://www.kaggle.com/whitepaper-day4"
date: "2025-12-31"
---

# Day 4: Agent Quality and Evaluation

## Why is it hard?
Agents are non-deterministic. "Correctness" is subjective.

## Evaluation Frameworks
- **LLM as a Judge**: Using a stronger model to grade the agent's output.
- **Trajectory Analysis**: Checking *how* the agent arrived at the answer, not just the answer itself.
- **Specific Metrics**: Faithfulness, Answer Relevance, Context Recall.
